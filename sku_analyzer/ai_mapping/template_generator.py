"""AI template generator for optimized token usage through structure reuse."""

from __future__ import annotations

import json
import logging
import time
from pathlib import Path
from typing import Any, Dict, Optional

from .models import AIProcessingConfig
from .prompts.templates import PromptTemplateManager
from .gemini_client import ModernGeminiClient


class AITemplateGenerator:
    """Generate reusable AI mapping templates to reduce token consumption."""
    
    def __init__(self, config: Optional[AIProcessingConfig] = None):
        """Initialize template generator.
        
        Args:
            config: AI processing configuration
        """
        self.config = config or AIProcessingConfig()
        self.logger = logging.getLogger(__name__)
        self.gemini_client = ModernGeminiClient(self.config)
        self.prompt_manager = PromptTemplateManager()
        
        # Performance tracking
        self.template_generation_stats = {
            "templates_generated": 0,
            "total_generation_time": 0.0,
            "average_generation_time": 0.0,
            "token_usage": {
                "template_generation_tokens": 0,
                "estimated_savings_per_parent": 0
            }
        }
    
    async def generate_mapping_template(
        self,
        mandatory_fields: Dict[str, Any],
        flat_file_metadata: Dict[str, Any],
        output_path: Path
    ) -> Dict[str, Any]:
        """Generate optimized AI mapping template.
        
        Args:
            mandatory_fields: Mandatory field definitions from step3
            flat_file_metadata: Metadata about source data structure
            output_path: Path to save template
            
        Returns:
            Template generation result with optimization metrics
        """
        start_time = time.time()
        self.logger.info("ðŸ—ï¸ Generating AI mapping template for token optimization")
        
        try:
            # Create optimized template prompt (60-80% smaller)
            template_prompt = self._create_template_generation_prompt(
                mandatory_fields, flat_file_metadata
            )
            
            # Generate template with AI
            template_structure = await self._generate_template_with_ai(template_prompt)
            
            # Optimize template for reuse
            optimized_template = self._optimize_template_for_reuse(
                template_structure, mandatory_fields
            )
            
            # Calculate token savings
            token_savings = self._calculate_token_savings(
                optimized_template, mandatory_fields
            )
            
            # Save template
            await self._save_template(optimized_template, output_path)
            
            # Update performance stats
            generation_time = time.time() - start_time
            self._update_generation_stats(generation_time, token_savings)
            
            result = {
                "template_path": str(output_path),
                "generation_time_ms": generation_time * 1000,
                "optimization_metrics": {
                    "estimated_token_reduction_percent": token_savings["reduction_percent"],
                    "tokens_saved_per_parent": token_savings["tokens_per_parent"],
                    "total_estimated_savings": token_savings["total_savings"]
                },
                "template_metadata": {
                    "parent_fields_count": len(optimized_template["parent_field_mappings"]),
                    "variant_fields_count": len(optimized_template["variant_field_mappings"]),
                    "constraint_rules_count": len(optimized_template["constraint_rules"])
                }
            }
            
            self.logger.info(
                f"âœ… Template generated: {token_savings['reduction_percent']:.1f}% token reduction expected"
            )
            
            return result
            
        except Exception as e:
            self.logger.error(f"Template generation failed: {e}")
            raise
    
    def _create_template_generation_prompt(
        self,
        mandatory_fields: Dict[str, Any],
        flat_file_metadata: Dict[str, Any]
    ) -> str:
        """Create optimized prompt for template generation.
        
        Args:
            mandatory_fields: Field definitions and constraints
            flat_file_metadata: Source data structure info
            
        Returns:
            Optimized template generation prompt
        """
        # Compact field analysis for AI understanding
        field_categories = self._categorize_fields(mandatory_fields)
        constraint_summary = self._summarize_constraints(mandatory_fields)
        
        prompt = f"""**TASK: Generate Reusable AI Mapping Template**

**OBJECTIVE**: Create a compact mapping template that can be reused across multiple parent groups to reduce token consumption by 60-80%.

**FIELD CATEGORIES**:
- **Parent Fields** (shared): {', '.join(field_categories['parent_fields'])}
- **Variant Fields** (per-SKU): {', '.join(field_categories['variant_fields'])}

**CONSTRAINT PATTERNS**:
{json.dumps(constraint_summary, indent=2)}

**SOURCE DATA STRUCTURE**:
```json
{json.dumps(flat_file_metadata, indent=2)}
```

**REQUIRED TEMPLATE OUTPUT**:
Create a JSON template with these sections:

1. **parent_field_mappings**: Compact rules for parent-level transformations
2. **variant_field_mappings**: Compact rules for variant-level transformations  
3. **constraint_rules**: Optimized constraint validation patterns
4. **data_extraction_patterns**: Source field â†’ target field mappings
5. **validation_schema**: Minimal validation requirements

**OPTIMIZATION REQUIREMENTS**:
- Use symbolic references instead of full field definitions
- Compress constraint validation into pattern rules
- Create reusable extraction patterns
- Minimize redundant information

Generate the optimized template now:"""

        return prompt
    
    def _categorize_fields(self, mandatory_fields: Dict[str, Any]) -> Dict[str, list]:
        """Categorize fields into parent vs variant types.
        
        Args:
            mandatory_fields: Field definitions
            
        Returns:
            Categorized field lists
        """
        parent_fields = [
            'feed_product_type', 'brand_name', 'outer_material_type', 
            'target_gender', 'age_range_description', 'bottoms_size_system',
            'bottoms_size_class', 'country_of_origin', 'department_name',
            'recommended_browse_nodes', 'standard_price', 'quantity',
            'main_image_url', 'fabric_type', 'list_price_with_tax'
        ]
        
        variant_fields = [
            'item_sku', 'external_product_id', 'external_product_id_type',
            'item_name', 'color_map', 'color_name', 'size_name', 'size_map'
        ]
        
        return {
            "parent_fields": parent_fields,
            "variant_fields": variant_fields
        }
    
    def _summarize_constraints(self, mandatory_fields: Dict[str, Any]) -> Dict[str, Any]:
        """Create compact constraint summary.
        
        Args:
            mandatory_fields: Field definitions with constraints
            
        Returns:
            Summarized constraints for template
        """
        constraint_patterns = {}
        
        for field_name, field_info in mandatory_fields.items():
            valid_values = field_info.get("valid_values", [])
            
            if valid_values:
                # Compress large constraint lists
                if len(valid_values) > 20:
                    constraint_patterns[field_name] = {
                        "type": "large_enum",
                        "count": len(valid_values),
                        "examples": valid_values[:3],
                        "pattern_type": self._detect_constraint_pattern(valid_values)
                    }
                else:
                    constraint_patterns[field_name] = {
                        "type": "small_enum", 
                        "values": valid_values
                    }
            else:
                constraint_patterns[field_name] = {"type": "free_text"}
        
        return constraint_patterns
    
    def _detect_constraint_pattern(self, values: list) -> str:
        """Detect pattern type in constraint values.
        
        Args:
            values: List of valid values
            
        Returns:
            Pattern type identifier
        """
        if all(v.isdigit() for v in values[:5]):
            return "numeric_codes"
        elif all(len(v) <= 10 for v in values[:5]):
            return "short_codes"
        elif "land" in str(values).lower() or "country" in str(values).lower():
            return "country_names"
        else:
            return "descriptive_text"
    
    async def _generate_template_with_ai(self, prompt: str) -> Dict[str, Any]:
        """Generate template structure using AI.
        
        Args:
            prompt: Template generation prompt
            
        Returns:
            Generated template structure
        """
        try:
            # Use Gemini to generate optimized template
            response = await self.gemini_client.generate_content(prompt)
            
            # Parse JSON response
            template_data = await self.gemini_client.validate_json_response(response)
            
            # Track token usage
            if hasattr(response, 'usage_metadata'):
                prompt_tokens = getattr(response.usage_metadata, 'prompt_token_count', 0)
                response_tokens = getattr(response.usage_metadata, 'candidates_token_count', 0)
                
                self.template_generation_stats["token_usage"]["template_generation_tokens"] = (
                    prompt_tokens + response_tokens
                )
            
            return template_data
            
        except Exception as e:
            self.logger.error(f"AI template generation failed: {e}")
            # Fallback to rule-based template
            return self._create_fallback_template()
    
    def _create_fallback_template(self) -> Dict[str, Any]:
        """Create fallback template when AI generation fails.
        
        Returns:
            Basic template structure
        """
        return {
            "parent_field_mappings": {
                "brand_name": {"source": "MANUFACTURER_NAME", "validation": "enum"},
                "feed_product_type": {"source": "fixed", "value": "pants"},
                "outer_material_type": {"source": "FVALUE_3_5", "validation": "enum"},
                "country_of_origin": {"source": "COUNTRY_OF_ORIGIN", "validation": "enum"}
            },
            "variant_field_mappings": {
                "item_sku": {"source": "SUPPLIER_PID", "validation": "none"},
                "size_name": {"source": "FVALUE_3_2", "validation": "none"},
                "color_name": {"source": "FVALUE_3_3", "validation": "none"},
                "external_product_id": {"source": "INTERNATIONAL_PID", "validation": "none"}
            },
            "constraint_rules": {
                "enum_validation": ["brand_name", "outer_material_type", "country_of_origin"],
                "fixed_values": {"feed_product_type": "pants"},
                "numeric_mapping": ["size_name", "size_map"]
            },
            "optimization_metadata": {
                "generation_method": "fallback_rules",
                "expected_token_reduction": 0.65
            }
        }
    
    def _optimize_template_for_reuse(
        self,
        template_structure: Dict[str, Any],
        mandatory_fields: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Optimize template for maximum reuse efficiency.
        
        Args:
            template_structure: Generated template structure
            mandatory_fields: Original field definitions
            
        Returns:
            Optimized template for reuse
        """
        # Add constraint reference tables (not full data)
        optimized = dict(template_structure)
        
        # Create constraint lookup tables instead of embedding full data
        optimized["constraint_references"] = {}
        for field_name, field_info in mandatory_fields.items():
            valid_values = field_info.get("valid_values", [])
            if valid_values and len(valid_values) > 5:
                # Reference only, full data loaded separately
                optimized["constraint_references"][field_name] = {
                    "constraint_type": "external_enum",
                    "value_count": len(valid_values),
                    "reference_key": f"mandatory_fields.{field_name}.valid_values"
                }
        
        # Add optimization metadata
        optimized["optimization_metadata"] = {
            "template_version": "v1.0",
            "generation_timestamp": time.time(),
            "reuse_strategy": "constraint_reference",
            "expected_token_reduction": 0.75
        }
        
        return optimized
    
    def _calculate_token_savings(
        self,
        optimized_template: Dict[str, Any],
        mandatory_fields: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Calculate expected token savings.
        
        Args:
            optimized_template: Generated template
            mandatory_fields: Original field definitions
            
        Returns:
            Token savings analysis
        """
        # Estimate current token usage per parent
        current_tokens_per_parent = (
            8000 +  # Prompt template
            len(json.dumps(mandatory_fields)) * 0.75 +  # ~0.75 tokens per char
            2000    # Product data
        )
        
        # Estimate optimized token usage
        template_size = len(json.dumps(optimized_template)) * 0.75
        optimized_tokens_per_parent = (
            2000 +  # Simplified mapping prompt
            500 +   # Template reference
            2000    # Product data (unchanged)
        )
        
        tokens_saved = current_tokens_per_parent - optimized_tokens_per_parent
        reduction_percent = (tokens_saved / current_tokens_per_parent) * 100
        
        # Assume 6 parent groups typical
        total_savings = tokens_saved * 6
        
        return {
            "current_tokens_per_parent": current_tokens_per_parent,
            "optimized_tokens_per_parent": optimized_tokens_per_parent,
            "tokens_per_parent": tokens_saved,
            "reduction_percent": reduction_percent,
            "total_savings": total_savings,
            "template_generation_cost": template_size
        }
    
    async def _save_template(
        self,
        template: Dict[str, Any],
        output_path: Path
    ) -> None:
        """Save generated template to file.
        
        Args:
            template: Template structure to save
            output_path: Output file path
        """
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with output_path.open('w', encoding='utf-8') as f:
            json.dump(template, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"ðŸ’¾ Template saved: {output_path}")
    
    def _update_generation_stats(
        self,
        generation_time: float,
        token_savings: Dict[str, Any]
    ) -> None:
        """Update template generation statistics.
        
        Args:
            generation_time: Time taken to generate template
            token_savings: Calculated token savings
        """
        self.template_generation_stats["templates_generated"] += 1
        self.template_generation_stats["total_generation_time"] += generation_time
        
        count = self.template_generation_stats["templates_generated"]
        self.template_generation_stats["average_generation_time"] = (
            self.template_generation_stats["total_generation_time"] / count
        )
        
        self.template_generation_stats["token_usage"]["estimated_savings_per_parent"] = (
            token_savings["tokens_per_parent"]
        )
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """Get template generation performance statistics.
        
        Returns:
            Performance statistics dictionary
        """
        return dict(self.template_generation_stats)


class OptimizedDataMapper:
    """Optimized data mapper using pre-generated templates."""
    
    def __init__(self, config: Optional[AIProcessingConfig] = None):
        """Initialize optimized mapper.
        
        Args:
            config: AI processing configuration
        """
        self.config = config or AIProcessingConfig()
        self.logger = logging.getLogger(__name__)
        self.gemini_client = ModernGeminiClient(self.config)
        
        # Performance tracking
        self.mapping_stats = {
            "parents_processed": 0,
            "total_mapping_time": 0.0,
            "average_mapping_time": 0.0,
            "token_usage": {
                "total_tokens_used": 0,
                "average_tokens_per_parent": 0.0
            }
        }
    
    async def map_parent_data_with_template(
        self,
        parent_sku: str,
        product_data: Dict[str, Any],
        template_path: Path,
        mandatory_fields: Dict[str, Any],
        output_path: Path
    ) -> Dict[str, Any]:
        """Map parent data using pre-generated template.
        
        Args:
            parent_sku: Parent SKU identifier
            product_data: Compressed product data
            template_path: Path to generated template
            mandatory_fields: Original mandatory fields (for validation)
            output_path: Output path for mapping result
            
        Returns:
            Mapping result with performance metrics
        """
        start_time = time.time()
        self.logger.info(f"ðŸŽ¯ Mapping parent {parent_sku} using template")
        
        try:
            # Load template
            template = await self._load_template(template_path)
            
            # Create optimized mapping prompt (much smaller)
            mapping_prompt = self._create_optimized_mapping_prompt(
                parent_sku, product_data, template
            )
            
            # Execute mapping with AI
            mapping_result = await self._execute_template_mapping(
                mapping_prompt, parent_sku
            )
            
            # Validate result against constraints
            validated_result = self._validate_mapping_result(
                mapping_result, mandatory_fields
            )
            
            # Save result
            await self._save_mapping_result(validated_result, output_path)
            
            # Update performance stats
            mapping_time = time.time() - start_time
            self._update_mapping_stats(mapping_time, mapping_prompt)
            
            result = {
                "parent_sku": parent_sku,
                "success": True,
                "mapping_time_ms": mapping_time * 1000,
                "output_file": str(output_path),
                "optimization_metrics": {
                    "template_used": str(template_path),
                    "prompt_size_reduction": self._calculate_prompt_reduction(mapping_prompt),
                    "constraint_validation": "template_based"
                }
            }
            
            self.logger.info(f"âœ… Parent {parent_sku} mapped in {mapping_time:.2f}s")
            return result
            
        except Exception as e:
            self.logger.error(f"Template-based mapping failed for {parent_sku}: {e}")
            raise
    
    async def _load_template(self, template_path: Path) -> Dict[str, Any]:
        """Load pre-generated template.
        
        Args:
            template_path: Path to template file
            
        Returns:
            Loaded template structure
        """
        with template_path.open('r', encoding='utf-8') as f:
            return json.load(f)
    
    def _create_optimized_mapping_prompt(
        self,
        parent_sku: str,
        product_data: Dict[str, Any],
        template: Dict[str, Any]
    ) -> str:
        """Create optimized mapping prompt using template.
        
        Args:
            parent_sku: Parent SKU identifier
            product_data: Source product data
            template: Pre-generated mapping template
            
        Returns:
            Optimized mapping prompt (60-80% smaller)
        """
        prompt = f"""**DATA MAPPING TASK** - Parent: {parent_sku}

**TEMPLATE-BASED MAPPING**: Use pre-defined structure and rules.

**MAPPING RULES** (from template):
```json
{json.dumps(template.get('parent_field_mappings', {}), indent=1)}
```

**VARIANT RULES** (from template):
```json
{json.dumps(template.get('variant_field_mappings', {}), indent=1)}
```

**SOURCE DATA**:
```json
{json.dumps(product_data, indent=1)}
```

**TASK**: Apply template rules to source data. Output exact JSON structure:
- Use parent_field_mappings for parent_data section
- Use variant_field_mappings for variance_data section  
- Apply constraint_rules for validation
- Include metadata with confidence score

Output mapped JSON now:"""

        return prompt
    
    async def _execute_template_mapping(
        self,
        prompt: str,
        parent_sku: str
    ) -> Dict[str, Any]:
        """Execute mapping using AI with template.
        
        Args:
            prompt: Optimized mapping prompt
            parent_sku: Parent SKU for context
            
        Returns:
            AI mapping result
        """
        try:
            response = await self.gemini_client.generate_content(prompt)
            mapping_data = await self.gemini_client.validate_json_response(response)
            
            # Track token usage
            if hasattr(response, 'usage_metadata'):
                prompt_tokens = getattr(response.usage_metadata, 'prompt_token_count', 0)
                response_tokens = getattr(response.usage_metadata, 'candidates_token_count', 0)
                total_tokens = prompt_tokens + response_tokens
                
                self.mapping_stats["token_usage"]["total_tokens_used"] += total_tokens
            
            return mapping_data
            
        except Exception as e:
            self.logger.error(f"Template mapping execution failed: {e}")
            raise
    
    def _validate_mapping_result(
        self,
        mapping_result: Dict[str, Any],
        mandatory_fields: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Validate mapping result against constraints.
        
        Args:
            mapping_result: AI-generated mapping result
            mandatory_fields: Original field constraints
            
        Returns:
            Validated mapping result
        """
        # Basic validation - more efficient than full constraint checking
        validated = dict(mapping_result)
        
        # Ensure required structure exists
        if "parent_data" not in validated:
            validated["parent_data"] = {}
        if "variance_data" not in validated:
            validated["variance_data"] = []
        if "metadata" not in validated:
            validated["metadata"] = {"mapping_confidence": 0.8}
        
        return validated
    
    async def _save_mapping_result(
        self,
        result: Dict[str, Any],
        output_path: Path
    ) -> None:
        """Save mapping result to file.
        
        Args:
            result: Mapping result to save
            output_path: Output file path
        """
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with output_path.open('w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
    
    def _calculate_prompt_reduction(self, prompt: str) -> float:
        """Calculate prompt size reduction percentage.
        
        Args:
            prompt: Optimized prompt
            
        Returns:
            Reduction percentage
        """
        # Estimate vs current ~28,000 token prompts
        current_estimate = 28000
        optimized_estimate = len(prompt) * 0.75  # ~0.75 tokens per char
        
        return ((current_estimate - optimized_estimate) / current_estimate) * 100
    
    def _update_mapping_stats(
        self,
        mapping_time: float,
        prompt: str
    ) -> None:
        """Update mapping performance statistics.
        
        Args:
            mapping_time: Time taken for mapping
            prompt: Mapping prompt used
        """
        self.mapping_stats["parents_processed"] += 1
        self.mapping_stats["total_mapping_time"] += mapping_time
        
        count = self.mapping_stats["parents_processed"]
        self.mapping_stats["average_mapping_time"] = (
            self.mapping_stats["total_mapping_time"] / count
        )
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """Get mapping performance statistics.
        
        Returns:
            Performance statistics dictionary
        """
        if self.mapping_stats["parents_processed"] > 0:
            self.mapping_stats["token_usage"]["average_tokens_per_parent"] = (
                self.mapping_stats["token_usage"]["total_tokens_used"] / 
                self.mapping_stats["parents_processed"]
            )
        
        return dict(self.mapping_stats)